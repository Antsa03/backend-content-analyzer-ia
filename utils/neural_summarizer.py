"""
R√©sumeur am√©lior√© utilisant les r√©seaux de neurones (Deep Learning).
Combine l'approche TF-IDF classique avec des mod√®les Transformer.
"""

from typing import Optional
import logging

from .text_cleaner import clean_text_for_web
from .header_detector import smart_clean_document

logger = logging.getLogger(__name__)

# Flag pour activer/d√©sactiver le mode Deep Learning
USE_NEURAL_MODEL = True

try:
    from models.neural_models import ModelManager

    model_manager = ModelManager()
    logger.info("‚úÖ Mod√®les neuronaux disponibles")
except ImportError as e:
    USE_NEURAL_MODEL = False
    logger.warning(f"‚ö†Ô∏è  Mod√®les neuronaux non disponibles: {e}")
    logger.info("üìä Utilisation du mode TF-IDF classique")


def generate_summary_neural(text: str, length: str = "medium") -> str:
    """
    G√©n√®re un r√©sum√© en utilisant un r√©seau de neurones Transformer.

    La longueur du r√©sum√© est calcul√©e dynamiquement en fonction du texte source :
    - short : r√©duction √† 80% (conserve 20% du texte)
    - medium : r√©duction √† 60% (conserve 40% du texte)
    - long : r√©duction √† 45% (conserve 55% du texte)

    Args:
        text: Texte source √† r√©sumer
        length: "short", "medium" ou "long"

    Returns:
        R√©sum√© g√©n√©r√© par le r√©seau de neurones
    """
    text = clean_text_for_web(text)
    text = smart_clean_document(text)

    if not USE_NEURAL_MODEL:
        # Fallback vers l'approche classique TF-IDF
        from .summarizer import generate_summary

        return generate_summary(text, length)

    try:
        # Calcul de la longueur du texte source en mots
        word_count = len(text.split())

        # Configuration bas√©e sur les pourcentages de r√©duction
        # short: garde 20% (r√©duit 80%)
        # medium: garde 40% (r√©duit 60%)
        # long: garde 55% (r√©duit 45%)
        reduction_config = {
            "short": {
                "reduction_percent": 0.80,  # R√©duction √† 80% (garde 20%)
                "keep_percent": 0.20,
                "num_beams": 5,
                "repetition_penalty": 1.5,  # ‚úÖ Augment√© de 1.2 √† 1.5
            },
            "medium": {
                "reduction_percent": 0.60,  # R√©duction √† 60% (garde 40%)
                "keep_percent": 0.40,
                "num_beams": 6,
                "repetition_penalty": 1.5,  # ‚úÖ Augment√© de 1.2 √† 1.5
            },
            "long": {
                "reduction_percent": 0.45,  # R√©duction √† 45% (garde 55%)
                "keep_percent": 0.55,
                "num_beams": 6,
                "repetition_penalty": 1.4,  # ‚úÖ Augment√© de 1.15 √† 1.4
            },
        }

        config = reduction_config.get(length, reduction_config["medium"])

        # Calcul dynamique des longueurs bas√© sur le pourcentage
        target_length = int(word_count * config["keep_percent"])

        # D√©finir min et max avec une marge de flexibilit√©
        min_length = max(30, int(target_length * 0.7))  # 70% de la cible minimum
        max_length = min(
            400, int(target_length * 1.2)
        )  # ‚úÖ R√©duit √† 400 max et marge √† 20%

        # S√©curit√© : limites absolues plus strictes
        min_length = max(min_length, 30)  # Au moins 30 mots
        max_length = min(max_length, 400)  # ‚úÖ Au plus 400 mots (r√©duit de 512)

        # S'assurer que min < max
        if min_length >= max_length:
            max_length = min_length + 50

        logger.info(
            f"üìä Texte source: {word_count} mots | "
            f"R√©duction: {int(config['reduction_percent']*100)}% | "
            f"Cible: {target_length} mots | "
            f"Plage: {min_length}-{max_length} mots"
        )

        # R√©cup√©ration du mod√®le neuronal (BARThez optimis√© pour le fran√ßais)
        summarizer = model_manager.get_summarizer(
            model_name="moussaKam/barthez-orangesum-abstract"
        )

        # G√©n√©ration du r√©sum√© avec les param√®tres calcul√©s dynamiquement
        summary = summarizer.generate_summary(
            text,
            max_length=max_length,
            min_length=min_length,
            num_beams=config["num_beams"],
            repetition_penalty=config["repetition_penalty"],
        )

        return clean_text_for_web(summary)

    except Exception as e:
        logger.error(f"‚ùå Erreur avec le mod√®le neuronal: {e}")
        logger.info("üîÑ Basculement vers TF-IDF classique")
        from .summarizer import generate_summary

        return generate_summary(text, length)


def generate_summary_hybrid(text: str, length: str = "medium") -> dict:
    """
    G√©n√®re deux r√©sum√©s : un avec Deep Learning et un avec TF-IDF.
    Permet de comparer les deux approches.

    Returns:
        Dict avec 'neural_summary' et 'tfidf_summary'
    """
    from .summarizer import generate_summary as generate_tfidf

    neural_summary = generate_summary_neural(text, length)
    tfidf_summary = generate_tfidf(text, length)

    return {
        "neural_summary": neural_summary,
        "tfidf_summary": tfidf_summary,
        "method_used": "hybrid",
    }
