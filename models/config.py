"""
Configuration des mod√®les de Deep Learning.
Permet de personnaliser les mod√®les utilis√©s et leurs param√®tres.
"""

# ==========================================================
# üéØ CONFIGURATION DES MOD√àLES
# ==========================================================

# Mod√®le pour le r√©sum√©
SUMMARIZER_CONFIG = {
    # Mod√®le principal (recommand√© pour le fran√ßais)
    # Options class√©es par qualit√© pour le fran√ßais :
    # 1. "moussaKam/barthez-orangesum-abstract" - MEILLEUR pour r√©sum√©s fran√ßais
    # 2. "plguillou/t5-base-fr-sum-cnndm" - T5 fran√ßais sp√©cialis√© r√©sum√©
    # 3. "facebook/mbart-large-50" - Multilingue (moins sp√©cialis√©)
    # 4. "google/mt5-base" - mT5 multilingue (plus l√©ger)
    "model_name": "moussaKam/barthez-orangesum-abstract",  # ‚úÖ Optimis√© pour r√©sum√© FR
    # Param√®tres de g√©n√©ration bas√©s sur POURCENTAGES DE R√âDUCTION
    # Les longueurs sont calcul√©es dynamiquement selon la taille du texte source
    "generation_params": {
        "short": {
            # R√©duction √† 80% (conserve 20% du texte original)
            "reduction_percent": 0.80,
            "keep_percent": 0.20,
            "num_beams": 5,
            "temperature": 0.85,
            "repetition_penalty": 1.2,
        },
        "medium": {
            # R√©duction √† 60% (conserve 40% du texte original)
            "reduction_percent": 0.60,
            "keep_percent": 0.40,
            "num_beams": 6,
            "temperature": 0.9,
            "repetition_penalty": 1.2,
        },
        "long": {
            # R√©duction √† 45% (conserve 55% du texte original)
            "reduction_percent": 0.45,
            "keep_percent": 0.55,
            "num_beams": 6,
            "temperature": 0.95,
            "repetition_penalty": 1.15,
        },
    },
    # Limites absolues pour la g√©n√©ration
    "length_limits": {
        "min_words": 30,  # Minimum absolu de mots dans un r√©sum√©
        "max_words": 512,  # Maximum absolu (limite du mod√®le)
        "flexibility_min": 0.7,  # Marge basse (70% de la cible)
        "flexibility_max": 1.3,  # Marge haute (130% de la cible)
    },
    # Param√®tres avanc√©s OPTIMIS√âS
    "advanced_params": {
        "length_penalty": 1.5,  # R√©duit de 2.0 pour plus de flexibilit√©
        "no_repeat_ngram_size": 3,  # √âvite les r√©p√©titions de 3-grams
        "do_sample": False,  # D√©terministe pour coh√©rence
    },
}

# Mod√®le pour la g√©n√©ration de quiz
QUIZ_GENERATOR_CONFIG = {
    # Mod√®le principal optimis√© pour le fran√ßais
    # Options class√©es par qualit√© pour le fran√ßais :
    # 1. "etalab-ia/camembert2-large-fquad" - CamemBERT fran√ßais QA (RECOMMAND√â)
    # 2. "lincoln/flaubert-mlsum-topic-classification" - FlauBERT fran√ßais
    # 3. "google/flan-t5-base" - Multilingue g√©n√©raliste (moins sp√©cialis√©)
    # 4. "mrm8488/t5-base-finetuned-question-generation-ap" - Anglais principalement
    "model_name": "google/flan-t5-base",  # Gard√© pour compatibilit√©, mais avec prompts FR optimis√©s
    # Prompts optimis√©s pour le fran√ßais
    "prompts_fr": {
        "question_with_answer": "G√©n√©rez une question en fran√ßais bas√©e sur cette information. R√©ponse attendue: {answer}. Contexte: {context}",
        "question_cloze": "Cr√©ez une question √† trou en fran√ßais pour tester cette connaissance: {context}",
        "question_what": "√Ä partir du texte suivant, posez une question commen√ßant par 'Quel', 'Quelle', 'Quels' ou 'Quelles': {context}",
        "question_why": "Formulez une question 'Pourquoi' bas√©e sur: {context}",
        "question_how": "Cr√©ez une question 'Comment' √† partir de: {context}",
        "distractor": "G√©n√©rez une r√©ponse incorrecte mais plausible en fran√ßais. Question: {question}. Bonne r√©ponse: {correct_answer}. Contexte: {context}",
    },
    # Param√®tres pour la g√©n√©ration de questions
    "question_params": {
        "max_length": 80,  # ‚úÖ Augment√© de 64 √† 80 pour questions fran√ßaises plus compl√®tes
        "min_length": 8,  # ‚úÖ Minimum de 8 tokens pour coh√©rence
        "num_beams": 6,  # ‚úÖ Augment√© de 4 √† 6 pour meilleure qualit√©
        "repetition_penalty": 1.3,  # ‚úÖ √âvite r√©p√©titions
        "no_repeat_ngram_size": 3,  # ‚úÖ Bloque trigrammes r√©p√©t√©s
        "length_penalty": 1.0,  # ‚úÖ Neutre pour longueur naturelle
        "do_sample": False,  # D√©terministe pour coh√©rence
    },
    # Param√®tres pour la g√©n√©ration de distracteurs
    "distractor_params": {
        "max_length": 40,  # ‚úÖ Augment√© de 32 √† 40 pour distracteurs fran√ßais
        "min_length": 3,  # ‚úÖ Minimum de 3 tokens
        "num_beams": 5,  # ‚úÖ Augment√© pour plus de diversit√©
        "repetition_penalty": 1.4,  # ‚úÖ Forte p√©nalit√© pour √©viter r√©p√©titions
        "do_sample": True,  # √âchantillonnage pour diversit√©
        "top_k": 40,  # ‚úÖ R√©duit de 50 √† 40 pour meilleure qualit√©
        "top_p": 0.92,  # ‚úÖ Ajust√© de 0.95 √† 0.92 pour plus de coh√©rence
        "temperature": 0.85,  # ‚úÖ Temp√©rature mod√©r√©e pour cr√©ativit√© contr√¥l√©e
    },
    # Validation de qualit√© des questions
    "quality_validation": {
        "min_question_length": 5,  # Minimum de mots dans une question
        "max_question_length": 30,  # Maximum de mots dans une question
        "min_distractor_length": 2,
        "require_question_mark": True,  # Force le point d'interrogation
        "forbidden_words": ["[UNK]", "<unk>", "undefined"],  # Mots interdits
        "french_question_starters": [  # D√©buts de questions valides en fran√ßais
            "quel",
            "quelle",
            "quels",
            "quelles",
            "qui",
            "que",
            "quoi",
            "o√π",
            "quand",
            "comment",
            "pourquoi",
            "combien",
            "lequel",
            "laquelle",
            "lesquels",
            "lesquelles",
            "est-ce",
            "peut-on",
            "doit-on",
            "faut-il",
            "dans",
            "selon",
            "√† partir",
            "compl√©tez",
        ],
    },
}

# ==========================================================
# ‚öôÔ∏è CONFIGURATION SYST√àME
# ==========================================================

SYSTEM_CONFIG = {
    # Activer/d√©sactiver le mode Deep Learning
    "use_neural_models": True,
    # Fallback vers TF-IDF si erreur
    "fallback_to_tfidf": True,
    # Device : "cuda", "cpu", ou "auto" (d√©tection automatique)
    "device": "auto",
    # Batch size pour traitement par lots (si impl√©ment√©)
    "batch_size": 1,
    # Cache des mod√®les
    "cache_models": True,  # Garde les mod√®les en m√©moire (Singleton)
    # Logging
    "log_level": "INFO",  # DEBUG, INFO, WARNING, ERROR
    # Timeouts
    "generation_timeout": 30,  # secondes
}

# ==========================================================
# üîß CONFIGURATION AVANC√âE (Optimisation)
# ==========================================================

OPTIMIZATION_CONFIG = {
    # Quantization (r√©duction de la pr√©cision pour √©conomiser m√©moire)
    "use_quantization": False,  # Requiert optimum library
    "quantization_bits": 8,  # 8 ou 4 bits
    # Mixed precision (pour GPU)
    "use_mixed_precision": True,  # float16 au lieu de float32
    # Gradient checkpointing (√©conomise m√©moire)
    "gradient_checkpointing": False,  # Utile seulement pour le fine-tuning
    # Number of threads (pour CPU)
    "num_threads": 4,
    # Max input length (tronquer les textes trop longs)
    "max_input_length": 1024,  # tokens
}

# ==========================================================
# üé® CONFIGURATION DES PROMPTS (Prompt Engineering)
# ==========================================================

PROMPT_CONFIG = {
    # Templates pour la g√©n√©ration de questions
    "question_templates": {
        "with_answer": "generate question: answer: {answer} context: {context}",
        "without_answer": "generate question: {context}",
        "mcq": "generate multiple choice question: context: {context}",
    },
    # Templates pour la g√©n√©ration de distracteurs
    "distractor_template": (
        "generate wrong answer: question: {question} "
        "correct answer: {correct_answer} context: {context}"
    ),
    # Langues support√©es
    "languages": ["fr", "en", "es", "de", "it"],
    "default_language": "fr",
}

# ==========================================================
# üìä CONFIGURATION DES M√âTRIQUES (√âvaluation)
# ==========================================================

METRICS_CONFIG = {
    # Activer le calcul de m√©triques
    "enable_metrics": False,  # D√©sactiv√© par d√©faut pour performance
    # M√©triques pour r√©sum√©s
    "summary_metrics": ["rouge", "bleu", "bertscore"],
    # M√©triques pour quiz
    "quiz_metrics": ["diversity", "difficulty", "relevance"],
    # Seuils de qualit√©
    "quality_thresholds": {
        "min_summary_length": 20,  # mots
        "max_summary_length": 500,  # mots
        "min_question_length": 5,  # mots
        "min_options": 4,
    },
}

# ==========================================================
# üöÄ PRESETS (Configurations pr√©d√©finies)
# ==========================================================

PRESETS = {
    "fast": {
        # Configuration optimis√©e pour vitesse
        "summarizer_model": "google/mt5-small",
        "quiz_model": "google/flan-t5-small",
        "num_beams": 2,
        "use_mixed_precision": True,
    },
    "balanced": {
        # Configuration √©quilibr√©e (par d√©faut)
        "summarizer_model": "facebook/mbart-large-50",
        "quiz_model": "google/flan-t5-base",
        "num_beams": 4,
        "use_mixed_precision": True,
    },
    "quality": {
        # Configuration optimis√©e pour qualit√©
        "summarizer_model": "facebook/mbart-large-50",
        "quiz_model": "google/flan-t5-large",
        "num_beams": 8,
        "use_mixed_precision": False,
    },
    "cpu_optimized": {
        # Configuration pour CPU (sans GPU)
        "summarizer_model": "google/mt5-base",
        "quiz_model": "google/flan-t5-base",
        "num_beams": 2,
        "use_mixed_precision": False,
        "num_threads": 8,
    },
}

# ==========================================================
# üîí CONFIGURATION DE S√âCURIT√â
# ==========================================================

SECURITY_CONFIG = {
    # Longueur maximale des entr√©es (protection contre DOS)
    "max_input_chars": 50000,  # ~10000 mots
    # Nombre maximum de questions par requ√™te
    "max_questions_per_request": 50,
    # Rate limiting (si impl√©ment√©)
    "rate_limit_enabled": False,
    "requests_per_minute": 60,
}
