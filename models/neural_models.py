"""
Mod√®les de Deep Learning bas√©s sur les r√©seaux de neurones Transformer
pour la g√©n√©ration de r√©sum√©s et de quiz.
"""

try:
    import torch
    from transformers import (
        AutoTokenizer,
        AutoModelForSeq2SeqLM,
        T5ForConditionalGeneration,
        pipeline,
    )

    TORCH_AVAILABLE = True
except (ImportError, OSError) as e:
    TORCH_AVAILABLE = False
    print(f"‚ö†Ô∏è  PyTorch non disponible: {e}")
    print("‚ÑπÔ∏è  Le syst√®me utilisera uniquement le mode TF-IDF classique.")
    print("‚ÑπÔ∏è  Pour activer le Deep Learning, installez Visual C++ Redistributable:")
    print("‚ÑπÔ∏è  https://aka.ms/vs/17/release/vc_redist.x64.exe")

from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)


class NeuralSummarizer:
    """
    R√©sumeur bas√© sur un r√©seau de neurones Transformer (architecture encoder-decoder).
    Utilise des mod√®les pr√©-entra√Æn√©s comme mBART ou T5 pour g√©n√©rer des r√©sum√©s abstractifs.
    """

    def __init__(self, model_name: str = "moussaKam/barthez-orangesum-abstract"):
        """
        Initialise le mod√®le de r√©sum√© neuronal.

        Args:
            model_name: Nom du mod√®le Hugging Face √† utiliser
                       - "moussaKam/barthez-orangesum-abstract" : BART fran√ßais (RECOMMAND√â)
                       - "plguillou/t5-base-fr-sum-cnndm" : T5 fran√ßais pour r√©sum√©s
                       - "facebook/mbart-large-50" : multilingue (fran√ßais inclus)
        """
        if not TORCH_AVAILABLE:
            raise ImportError(
                "PyTorch n'est pas disponible. Installez Visual C++ Redistributable."
            )

        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üöÄ Chargement du mod√®le de r√©sum√© neuronal: {model_name}")
        logger.info(f"üñ•Ô∏è  Device utilis√©: {self.device}")

        try:
            # Chargement avec use_fast=False pour √©viter les erreurs de tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                use_fast=False,  # ‚úÖ Force le tokenizer lent (compatible SentencePiece)
            )
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            self.model.to(self.device)
            self.model.eval()  # Mode √©valuation (pas d'entra√Ænement)
            logger.info("‚úÖ Mod√®le de r√©sum√© charg√© avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            raise

    def generate_summary(
        self,
        text: str,
        max_length: int = 250,
        min_length: int = 80,
        num_beams: int = 6,
        repetition_penalty: float = 1.2,
    ) -> str:
        """
        G√©n√®re un r√©sum√© abstractif en utilisant le r√©seau de neurones.

        Args:
            text: Texte source √† r√©sumer
            max_length: Longueur maximale du r√©sum√© g√©n√©r√©
            min_length: Longueur minimale du r√©sum√© g√©n√©r√©
            num_beams: Nombre de beams pour la recherche (beam search)
            repetition_penalty: P√©nalit√© pour les r√©p√©titions (>1 = moins de r√©p√©titions)

        Returns:
            R√©sum√© g√©n√©r√© par le mod√®le
        """
        if not text or len(text.strip()) < 50:
            return text

        try:
            # Nettoyage et pr√©paration du texte
            text = text.strip()

            # Tokenisation du texte d'entr√©e avec troncature intelligente
            inputs = self.tokenizer(
                text,
                max_length=512,  # ‚úÖ R√©duit √† 512 pour √©viter les hallucinations
                truncation=True,
                padding="max_length",
                return_tensors="pt",
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # G√©n√©ration du r√©sum√© avec param√®tres optimis√©s pour BARThez
            with torch.no_grad():
                summary_ids = self.model.generate(
                    inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_length=max_length,
                    min_length=min_length,
                    num_beams=num_beams,
                    repetition_penalty=repetition_penalty,
                    no_repeat_ngram_size=4,  # ‚úÖ Augment√© de 3 √† 4 pour √©viter plus de r√©p√©titions
                    length_penalty=1.2,  # ‚úÖ R√©duit pour favoriser r√©sum√©s plus courts et coh√©rents
                    do_sample=False,  # D√©terministe (pas de temperature)
                    forced_eos_token_id=self.tokenizer.eos_token_id,  # ‚úÖ Force la fin propre
                )

            # D√©codage du r√©sultat
            summary = self.tokenizer.decode(
                summary_ids[0],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True,
            )

            # ‚úÖ Validation et nettoyage du r√©sum√©
            summary = self._clean_and_validate_summary(summary.strip(), text)

            return summary

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration du r√©sum√©: {e}")
            # Fallback : retourne les N premiers caract√®res
            return text[: max_length * 5]

    def _clean_and_validate_summary(self, summary: str, original_text: str) -> str:
        """
        Nettoie et valide le r√©sum√© g√©n√©r√© pour d√©tecter les hallucinations.

        Args:
            summary: R√©sum√© g√©n√©r√© par le mod√®le
            original_text: Texte source original

        Returns:
            R√©sum√© nettoy√© et valid√©
        """
        if not summary or len(summary.strip()) < 10:
            return original_text[:500]

        # D√©tecter les r√©p√©titions excessives de mots
        words = summary.split()

        # Si le r√©sum√© est trop long par rapport √† l'original, c'est suspect
        if len(words) > len(original_text.split()) * 0.8:
            logger.warning("‚ö†Ô∏è  R√©sum√© trop long, troncature")
            summary = " ".join(words[: len(original_text.split()) // 2])

        # D√©tecter les r√©p√©titions de patterns (ex: "K K K K")
        from collections import Counter

        word_counts = Counter(words)

        # Si un mot (hors mots courants) appara√Æt plus de 5 fois, c'est suspect
        common_words = {
            "le",
            "la",
            "les",
            "de",
            "des",
            "un",
            "une",
            "et",
            "ou",
            "√†",
            "du",
            "en",
        }
        suspicious = False
        for word, count in word_counts.items():
            if word.lower() not in common_words and count > 5:
                logger.warning(f"‚ö†Ô∏è  R√©p√©tition excessive d√©tect√©e: '{word}' x{count}")
                suspicious = True
                break

        # Si r√©p√©titions suspectes, tronquer au premier probl√®me
        if suspicious:
            cleaned_sentences = []
            sentences = summary.split(".")

            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue

                # V√©rifier si la phrase contient des r√©p√©titions anormales
                sentence_words = sentence.split()
                if len(sentence_words) < 3:
                    continue

                # V√©rifier les trigrammes r√©p√©t√©s
                has_repetition = False
                for i in range(len(sentence_words) - 2):
                    trigram = " ".join(sentence_words[i : i + 3])
                    if sentence.count(trigram) > 1:
                        has_repetition = True
                        break

                if not has_repetition:
                    cleaned_sentences.append(sentence)
                else:
                    logger.warning(
                        f"‚ö†Ô∏è  Phrase avec r√©p√©titions ignor√©e: {sentence[:50]}..."
                    )
                    break  # Arr√™ter d√®s qu'on trouve une phrase suspecte

            if cleaned_sentences:
                summary = ". ".join(cleaned_sentences) + "."
            else:
                # Si tout est suspect, fallback sur le d√©but du texte original
                logger.error(
                    "‚ùå R√©sum√© compl√®tement incoh√©rent, utilisation du texte source"
                )
                return original_text[:500] + "..."

        return summary.strip()


class NeuralQuizGenerator:
    """
    G√©n√©rateur de quiz bas√© sur un r√©seau de neurones Transformer (T5).
    G√©n√®re des questions et r√©ponses √† partir du texte source.
    """

    def __init__(self, model_name: str = "google/flan-t5-base"):
        """
        Initialise le mod√®le de g√©n√©ration de questions.

        Args:
            model_name: Nom du mod√®le Hugging Face √† utiliser
                       - "google/flan-t5-base" : T5 multilingue optimis√©
                       - "mrm8488/t5-base-finetuned-question-generation-ap" : T5 pour QG
        """
        if not TORCH_AVAILABLE:
            raise ImportError(
                "PyTorch n'est pas disponible. Installez Visual C++ Redistributable."
            )

        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"üöÄ Chargement du mod√®le de g√©n√©ration de quiz: {model_name}")
        logger.info(f"üñ•Ô∏è  Device utilis√©: {self.device}")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = T5ForConditionalGeneration.from_pretrained(model_name)
            self.model.to(self.device)
            self.model.eval()
            logger.info("‚úÖ Mod√®le de quiz charg√© avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            raise

    def generate_question(
        self,
        context: str,
        answer: Optional[str] = None,
        max_length: int = 80,
    ) -> str:
        """
        G√©n√®re une question en fran√ßais √† partir d'un contexte et optionnellement d'une r√©ponse.

        Args:
            context: Texte contexte en fran√ßais
            answer: R√©ponse cible (optionnel)
            max_length: Longueur max de la question

        Returns:
            Question g√©n√©r√©e en fran√ßais
        """
        try:
            # Prompt engineering optimis√© pour le fran√ßais
            if answer:
                # Prompt fran√ßais structur√© pour T5
                input_text = (
                    f"G√©n√©rez une question en fran√ßais bas√©e sur cette information. "
                    f"R√©ponse attendue: {answer}. Contexte: {context}"
                )
            else:
                # Prompt alternatif sans r√©ponse
                input_text = f"Posez une question pertinente en fran√ßais √† partir du texte suivant: {context}"

            inputs = self.tokenizer(
                input_text,
                max_length=512,
                truncation=True,
                padding="max_length",
                return_tensors="pt",
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                question_ids = self.model.generate(
                    inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_length=max_length,
                    min_length=8,  # ‚úÖ Minimum pour coh√©rence
                    num_beams=6,  # ‚úÖ Augment√© pour meilleure qualit√©
                    repetition_penalty=1.3,  # ‚úÖ √âvite r√©p√©titions
                    no_repeat_ngram_size=3,  # ‚úÖ Bloque trigrammes r√©p√©t√©s
                    length_penalty=1.0,  # ‚úÖ Neutre
                    do_sample=False,  # D√©terministe
                )

            question = self.tokenizer.decode(
                question_ids[0],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True,
            )

            # ‚úÖ Validation et nettoyage de la question
            question = self._validate_and_clean_question(
                question.strip(), context, answer
            )

            return question

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration de question: {e}")
            # Fallback intelligent en fran√ßais
            if answer:
                return f"Quel est le terme manquant : {context[:80]}... (R√©ponse: ______) ?"
            return f"Quelle information peut-on extraire de : {context[:60]}... ?"

    def generate_distractor(
        self,
        question: str,
        correct_answer: str,
        context: str,
    ) -> str:
        """
        G√©n√®re un distracteur (mauvaise r√©ponse plausible) en fran√ßais pour une question.

        Args:
            question: La question en fran√ßais
            correct_answer: La bonne r√©ponse
            context: Le contexte en fran√ßais

        Returns:
            Un distracteur plausible en fran√ßais
        """
        try:
            # Prompt fran√ßais optimis√© pour distracteurs
            input_text = (
                f"G√©n√©rez une r√©ponse incorrecte mais plausible en fran√ßais. "
                f"Question: {question} "
                f"Bonne r√©ponse: {correct_answer} "
                f"Contexte: {context[:200]}"
            )

            inputs = self.tokenizer(
                input_text,
                max_length=512,
                truncation=True,
                padding="max_length",
                return_tensors="pt",
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                distractor_ids = self.model.generate(
                    inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_length=40,  # ‚úÖ Augment√© pour distracteurs fran√ßais
                    min_length=3,  # ‚úÖ Minimum de coh√©rence
                    num_beams=5,  # ‚úÖ Augment√© pour qualit√©
                    repetition_penalty=1.4,  # ‚úÖ Forte p√©nalit√©
                    do_sample=True,  # √âchantillonnage pour diversit√©
                    top_k=40,  # ‚úÖ R√©duit pour meilleure qualit√©
                    top_p=0.92,  # ‚úÖ Ajust√© pour coh√©rence
                    temperature=0.85,  # ‚úÖ Cr√©ativit√© contr√¥l√©e
                )

            distractor = self.tokenizer.decode(
                distractor_ids[0],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True,
            )

            # ‚úÖ Validation du distracteur
            distractor = self._validate_distractor(distractor.strip(), correct_answer)

            return distractor

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration de distracteur: {e}")
            # Fallback intelligent en fran√ßais
            fallbacks = [
                "Autre r√©ponse possible",
                "Information non mentionn√©e",
                "Donn√©e absente du contexte",
            ]
            import random

            return random.choice(fallbacks)

    def _validate_and_clean_question(
        self, question: str, context: str, answer: Optional[str]
    ) -> str:
        """
        Valide et nettoie une question g√©n√©r√©e en fran√ßais.

        Args:
            question: Question g√©n√©r√©e
            context: Contexte source
            answer: R√©ponse attendue

        Returns:
            Question valid√©e et nettoy√©e
        """
        if not question or len(question.strip()) < 5:
            # Fallback si question vide
            if answer:
                return (
                    f"Quel est le terme manquant dans ce contexte : {context[:80]}... ?"
                )
            return f"Quelle information peut-on extraire du texte suivant ?"

        # Nettoyage de base
        question = question.strip()

        # V√©rifier les mots interdits
        forbidden = ["[UNK]", "<unk>", "undefined", "null"]
        for word in forbidden:
            if word.lower() in question.lower():
                logger.warning(f"‚ö†Ô∏è  Mot interdit d√©tect√© dans la question: {word}")
                if answer:
                    return f"Quelle est la r√©ponse correcte concernant {answer} dans ce contexte ?"
                return f"Quelle information principale est pr√©sente dans ce passage ?"

        # S'assurer qu'il y a un point d'interrogation
        if not question.endswith("?"):
            # V√©rifier si c'est bien une question en fran√ßais
            question_starters = [
                "quel",
                "quelle",
                "quels",
                "quelles",
                "qui",
                "que",
                "quoi",
                "o√π",
                "quand",
                "comment",
                "pourquoi",
                "combien",
            ]
            starts_with_question = any(
                question.lower().startswith(starter) for starter in question_starters
            )
            if starts_with_question:
                question = question.rstrip(".") + "?"

        # Limiter la longueur (max 200 caract√®res)
        if len(question) > 200:
            question = question[:197] + "...?"

        # Capitaliser le d√©but
        if question and question[0].islower():
            question = question[0].upper() + question[1:]

        return question

    def _validate_distractor(self, distractor: str, correct_answer: str) -> str:
        """
        Valide un distracteur g√©n√©r√©.

        Args:
            distractor: Distracteur g√©n√©r√©
            correct_answer: R√©ponse correcte (ne doit pas √™tre identique)

        Returns:
            Distracteur valid√©
        """
        if not distractor or len(distractor.strip()) < 2:
            return "R√©ponse alternative"

        distractor = distractor.strip()

        # V√©rifier qu'il n'est pas identique √† la bonne r√©ponse
        if distractor.lower() == correct_answer.lower():
            logger.warning(f"‚ö†Ô∏è  Distracteur identique √† la bonne r√©ponse")
            return f"Non-{correct_answer}"

        # Nettoyer les caract√®res sp√©ciaux inutiles
        distractor = distractor.strip(".,;:!?")

        # Limiter la longueur (max 100 caract√®res)
        if len(distractor) > 100:
            distractor = distractor[:97] + "..."

        return distractor


class ModelManager:
    """
    Gestionnaire centralis√© des mod√®les neuronaux.
    Impl√©mente le pattern Singleton pour √©viter de charger plusieurs fois les mod√®les.
    """

    _instance = None
    _summarizer: Optional[NeuralSummarizer] = None
    _quiz_generator: Optional[NeuralQuizGenerator] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def get_summarizer(
        self, model_name: str = "moussaKam/barthez-orangesum-abstract"
    ) -> NeuralSummarizer:
        """Retourne l'instance du r√©sumeur neuronal (singleton)."""
        if self._summarizer is None:
            self._summarizer = NeuralSummarizer(model_name)
        return self._summarizer

    def get_quiz_generator(
        self, model_name: str = "google/flan-t5-base"
    ) -> NeuralQuizGenerator:
        """Retourne l'instance du g√©n√©rateur de quiz neuronal (singleton)."""
        if self._quiz_generator is None:
            self._quiz_generator = NeuralQuizGenerator(model_name)
        return self._quiz_generator

    def unload_models(self):
        """Lib√®re la m√©moire en d√©chargeant les mod√®les."""
        if self._summarizer:
            del self._summarizer
            self._summarizer = None
        if self._quiz_generator:
            del self._quiz_generator
            self._quiz_generator = None
        if TORCH_AVAILABLE and torch.cuda.is_available():
            torch.cuda.empty_cache()
        logger.info("üóëÔ∏è  Mod√®les d√©charg√©s de la m√©moire")
